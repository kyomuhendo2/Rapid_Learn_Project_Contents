{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "smpwYCCJLr0W"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Gx8S6XT-Ou0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb475bea-6890-4ada-c13b-e14a43ab5835"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# TASK 1: Readinging Files from Google Drive & Data Preprocessing\n",
        "# ------------------------------\n",
        "# Defining the folder containing the .txt files on Google Drive\n",
        "text_data = '/content/text_data/'\n",
        "\n",
        "# Function to extract label from filename.\n",
        "# For example, if file names start with \"positive\" or \"negative\"\n",
        "def extract_label(filename):\n",
        " base = os.path.basename(filename)\n",
        " if base.startswith('positive'):\n",
        "    return 'positive'\n",
        " elif base.startswith('negative'):\n",
        "    return 'negative'\n",
        " else:\n",
        "    return 'unknown'\n",
        "\n",
        "# Function to clean and normalize text\n",
        "def clean_text(text):\n",
        " \"\"\"\n",
        " Clean and normalize text.\n",
        " Modify this function if your local language requires special handling\n",
        " (e.g., accented letters, diacritics, or specific punctuation).\n",
        " This example lowercases the text and removes non-alphabetic characters.\n",
        " \"\"\"\n",
        " text = text.lower()\n",
        " text = re.sub(r'[^a-z\\s]', '', text)\n",
        " text = re.sub(r'\\s+', ' ', text).strip()\n",
        " return text\n",
        "\n",
        "# Read all .txt files from the specified folder using glob\n",
        "file_paths = glob.glob(os.path.join(text_data, '*.txt'))\n",
        "\n",
        "print(f\"Found {len(file_paths)} text files.\")\n",
        "\n",
        "# Lists to store document texts and corresponding labels\n",
        "documents = []\n",
        "labels = []\n",
        "for file_path in file_paths:\n",
        " with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        " text = clean_text(text)\n",
        " documents.append(text)\n",
        " labels.append(extract_label(file_path))\n",
        "\n",
        "# Create a DataFrame from the documents and labels\n",
        "data = pd.DataFrame({'text': documents, 'label': labels})\n",
        "print(\"Data preview:\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxSeYV_FPHzP",
        "outputId": "55ed1716-6fd9-4418-e62d-5126fd1ae9df"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 text files.\n",
            "Data preview:\n",
            "                                                text     label\n",
            "0  bulabe yakwaatibwa naayisibwa bubi obuzibu ku ...  negative\n",
            "1  obuto bwe nokusoma kwe jennifer nansubuga maku...   unknown\n",
            "2  yazaalibwa era yakulira kampala omukozi wa bba...  positive\n",
            "3  yazaalibwa era yakulira kampala omukozi wa bba...  positive\n",
            "4  bulabe yakwaatibwa naayisibwa bubi obuzibu ku ...  negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------\n",
        "# TASK 2: Feature Extraction\n",
        "# ------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer to extract unigrams and bigrams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "features = vectorizer.fit_transform(data['text'])\n",
        "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n"
      ],
      "metadata": {
        "id": "OHa-eAukaOFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ed4051-e29a-4b48-f30a-7baedb8dded7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 1402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# TASK 3: Model Implementation with Multinomial Naive Bayes\n",
        "# ------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Split the data into training and test sets (optionally, you can create a development set)\n",
        "X = data['text']\n",
        "y = data['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        " X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "\n",
        "# Build a pipeline that combines TF-IDF vectorization with the multinomial naive Bayes classifier.\n",
        "# MultinomialNB applies Laplace (add-one) smoothing by default (alpha=1.0).\n",
        "\n",
        "pipeline = Pipeline([\n",
        " ('tfidf', TfidfVectorizer(ngram_range=(1, 2), lowercase=True)),\n",
        " ('nb', MultinomialNB(alpha=1.0))\n",
        "])\n",
        "\n",
        "# Train the naive Bayes classifier\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"Naive Bayes model training completed.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNFU1ukDgkZW",
        "outputId": "71dec11c-a0f1-49c3-9dbb-b6dabfbf7552"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# TASK 4: Model Evaluation and Analysis\n",
        "# ------------------------------\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "# Predict labels on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Accuracy on Test Set:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKRbFXYyrTvk",
        "outputId": "498399e6-e4db-40eb-a367-e5e6f0efeddc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test Set: 1.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00         1\n",
            "    positive       1.00      1.00      1.00         1\n",
            "     unknown       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Examine top discriminative features for each class\n",
        "# Retrieve the fitted vectorizer and classifier from the pipeline\n",
        "vectorizer = pipeline.named_steps['tfidf']\n",
        "classifier = pipeline.named_steps['nb']\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "def print_top_features(class_index, n=10):\n",
        " # For multinomial naive Bayes, the log probabilities for features can be found in feature_log_prob_\n",
        " coef = classifier.feature_log_prob_[class_index]\n",
        " # Sort features by their log probability contribution\n",
        " topn = sorted(zip(coef, feature_names), reverse=True)[:n]\n",
        " print(f\"\\nTop features for class '{classifier.classes_[class_index]}':\")\n",
        " for log_prob, feature in topn:\n",
        "  print(f\"{feature}: {log_prob:.4f}\")\n",
        "for idx in range(len(classifier.classes_)):\n",
        "  print_top_features(idx, n=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ5uYpdjrll8",
        "outputId": "a0c2dbc6-2067-4af0-f95e-84afe4898b35"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top features for class 'negative':\n",
            "sitaani gwegwasinga: -6.9228\n",
            "gwegwasinga ebyokugwa: -6.9228\n",
            "ebyokugwa lwebbula: -6.9228\n",
            "sitaani: -6.9907\n",
            "lwebbula: -6.9907\n",
            "gwegwasinga: -6.9907\n",
            "ebyokugwa: -6.9907\n",
            "lwebbula sitaani: -7.0186\n",
            "bwongo baayawukana: -7.1245\n",
            "bulabe yakwaatibwa: -7.1245\n",
            "\n",
            "Top features for class 'positive':\n",
            "yagisomera yakola: -6.9996\n",
            "okuttibwa yagisomera: -6.9996\n",
            "diguli okusomesa: -6.9996\n",
            "yataasibwa okuttibwa: -7.0530\n",
            "yataasibwa: -7.0530\n",
            "yakola diguli: -7.0530\n",
            "yakola: -7.0530\n",
            "yagisomera: -7.0530\n",
            "okuttibwa: -7.0530\n",
            "okusomesa: -7.0530\n",
            "\n",
            "Top features for class 'unknown':\n",
            "mu: -6.7491\n",
            "ku: -7.0095\n",
            "nga: -7.0666\n",
            "makumbi: -7.1078\n",
            "era: -7.1086\n",
            "nti: -7.1399\n",
            "nnyo: -7.1508\n",
            "the: -7.1619\n",
            "afirika: -7.1619\n",
            "okuva: -7.1731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# (Optional Bonus Task) Predict New Documents\n",
        "# ------------------------------\n",
        "# Predict labels for new sample documents (adjust the sample texts to your local language)\n",
        "new_documents = [\n",
        " \"kasule alina obugaga bungi\",\n",
        " \"kasule yegatta ku gavumenti\",\n",
        " \"obuzibu we bwava kuyomba\"\n",
        "]\n",
        "new_predictions = pipeline.predict(new_documents)\n",
        "print(\"\\nPredictions for new documents:\")\n",
        "for doc, pred in zip(new_documents, new_predictions):\n",
        " print(f\"Text: {doc}\\nPredicted Label: {pred}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnAnH65vrnAk",
        "outputId": "e62eb6ee-3e31-4a5e-e0d3-d2bf9867c67e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for new documents:\n",
            "Text: kasule alina obugaga bungi\n",
            "Predicted Label: positive\n",
            "\n",
            "Text: kasule yegatta ku gavumenti\n",
            "Predicted Label: unknown\n",
            "\n",
            "Text: obuzibu we bwava kuyomba\n",
            "Predicted Label: negative\n",
            "\n"
          ]
        }
      ]
    }
  ]
}